---
title: OpenCaseStudies Template
author: Open Case Study Team
output:
  html_document:
    md_extensions: -startnum
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

# Motivation 

In this case study, the goal is to read in the data 
from the first page of PDF reports, 
which gives exposure counts for calls to the 
[Maryland Poison Center](https://www.mdpoison.com/). 
Finally we would like to create a tidy and clean dataset of calls
to the Maryland Poison Center (MPC) based on reports 
form 2006 to 2008 of all counties(totally 312 files). 
Then we could visualize some information by animation along time horizons.

<center>
![](MPCmap.gif)
</center>


In previous open case studies, we have 
dealt with reading data from excel, csv or sas. 
There is a special format we have not tried---PDF,
which have tremendous adoption rates and 
became ubiquitous in today's work environment. 
Their ability to be viewed from a broad range of 
environments makes them especially appealing 
for exchanging important data. For example, 
many hedge funds try to create value from mining of news 
and various official filings, thus it is important 
to extract key information and update their investment method. 
Also, a vast amount of new information related to 
government policy appears constantly, with immediate impact 
on our life. Monitoring such information in real time 
is important for government organization 
but out of reach of the individual.  


But it is always hard to extract data from pdfs 
and convert them to a neat excel or csv format. 
Why? First, pdfs are scanned in files and don't contain
any 'selectable' text. While those documents are 
easily readable for humans, computers are not capable to 
understand the scanned image text. Of course you could do 
the simple copy and paste, but this is pretty time-consuming 
when you need to deal with hundreds or even thousands of pdfs. 
In addition, after reading PDFs, what you would get will be
plenty of strings, which usually contain unstructured or 
semi-structured data. Therefore, it is necessary to 
learn the basics of how string works. 

<center>
![](plot/PDF.png)
</center>


In this case study, we will focus on how to 
extract data from pdf documents provided by 
Maryland Poison Center, using the R package `pdftools`, 
and transform the original messy form into 
a tidy csv with package `stringr`. The final 
step is to visualize feature `Total human exposure`
with package `gganimate`.

The libraries used in this study are listed 
in the following table, along with their purpose 
in this particular case study:

|Library|Purpose|
|---|-------------------------------------------------------------------------------------------|
|`kableExtra`|Helps with building common complex tables and manipulating table styles; creates nice-looking HTML tables|
|`tidyverse`|A coherent system of packages for data manipulation, exploration and visualization |
|`pdftools`| Text extraction, rendering and converting of PDF documents|
|`stringr`|A consistent, simple and easy to use set of wrappers for common string operations|
|`readr`|Provide a fast and friendly way to read rectangular data, such as 'csv'|
|`maps`|Display of maps|
|`ggplot2`|Create elegant data visualisations using the grammar of graphics|

In order to run this code please ensure you have these packages installed. 

```{r packages}
library(tidyverse)
library(dplyr)
library(pdftools)
library(purrr)
library(stringr)
library(readr)
library(maps)
library(ggplot2)
library(gganimate)
```

The learning objectives for this case study include:

  * extract data from pdf files
  * string manipulation
  * animated map creation

# What is the data? 

All data files are provided by 
[the Maryland Poison Center (MPC)](https://www.mdpoison.com/), 
which aims to decrease the cost and 
complexity of poisoning and overdose care while 
maintaining and improving patient outcomes.
 
Annual reports about calls to the MPC can be accessed from 
[this page](https://www.mdpoison.com/factsandreports/), 
which includes information for all states and individual counties. 
We would like to focus on each individual county and 
they all have the same format like this:

<center>
![](plot/format.png)
</center>


Seemingly a burdensome task? You may feel more clear after looking at our plan:

* Download  all 312 county pdf files 
(24 counties by 13 years) for 2006-2018. 

* Write a function to extract data from 
any pdf document with this structure 
(all MPC county reports have this same structure 
from 2006-2018). A sample document (Allegany County 2018) 
will be used as example to display the extraction process.

* Test this function on two additional counties 
(Prince George's County, Talbot County) for 2018; 
update function as needed.

* Attempt to extract data from all 312 pdfs, 
and modify the function as needed to handle any error.

* Clean the resulting data set with regular expression.

# Data import

Call reports from all 24 Maryland counties equivalents 
for the years of 2006 - 2018 need to be downloaded. 
A sample pdf document can be found [here](https://www.mdpoison.com/media/SOP/mdpoisoncom/factsandreports/reports/countypdf2018/Allegany%20County%20Statistical%20Report%202018.pdf)

The general link looks like this:
https://www.mdpoison.com/media/SOP/mdpoisoncom/factsandreports/reports/countypdf**year**/**County%20Name%20**Statistical%20Report%20**year**.pdf

where **County%20Name%20** is the name of the county 
with %20 as spaces and **year** is the year of the data.  
County names with punctuation (periods, apostrophes) 
drop the punctuation in the link name. 

We can go back through 2006; reports prior to 2006 
have a different format so we will skip them for now.  

The 2016 links are a little bit different:
https://www.mdpoison.com/media/SOP/mdpoisoncom/factsandreports/reports/**county-pdf-2016**/Allegany%20County%20Statistical%20Report%202016.pdf

so we will have to account for that:

```{r createLinks}
countyNames <- c("Allegany County", "Anne Arundel County", 
                 "Baltimore City", "Baltimore County", 
                 "Calvert County", "Caroline County", 
                 "Carroll County", "Cecil County", 
                 "Charles County", "Dorchester County", 
                 "Frederick County", "Garrett County", 
                 "Harford County", "Howard County", "Kent County", 
                 "Montgomery County", "Prince Georges County", 
                 "Queen Annes County", "Somerset County", 
                 "St Marys County", "Talbot County", "Washington County", 
                 "Wicomico County", "Worcester County")
years <- 2006:2018
```

Now we create all of the links for each county/year combination:
```{r}
links <- NULL
files <- NULL
for (i in years) {
  for (j in countyNames) {
    countyNameForLink <- 
      paste(unlist(strsplit(j, " ")), collapse="%20")
    if (i != 2016) {
      tempLink <- paste0("https://www.mdpoison.com/media/SOP/mdpoisoncom/factsandreports/reports/countypdf",i,"/",
      countyNameForLink,"%20Statistical%20Report%20",i,".pdf")} else {
      tempLink <- paste0("https://www.mdpoison.com/media/SOP/mdpoisoncom/factsandreports/reports/county-pdf-",i,"/",
      countyNameForLink,"%20Statistical%20Report%20",i,".pdf")}
    tempFile <- paste0(j," Statistical Report ", i,".pdf")
    links <- c(links, tempLink)
    files <- c(files, tempFile)
  }
}
```

Download and save documents in a `data` subfolder. Remember to create this subfolder first otherwise you would receive error:

```{r downloadDocuments, eval=FALSE}
for (i in 1:length(links)) {
  download.file(links[i], paste0("./data/",files[i]))
}
```

```{r include=FALSE}
files = list.files(path='./data',pattern="*.pdf")
```

Next, "Allegany County Statistical Report 2018.pdf" 
will be used as an example to show each wrangling step.

# Data wrangling 

The plot below shows what we will perform generally:

<center>
![](plot/agn18.png)
</center>


## Step 1: Read and Extract 

In order to read data from pdf, we will use function [pdf_data()](https://www.rdocumentation.org
/packages/pdftools/versions/2.2/topics/pdftools). 
This function will return one data frame per page, 
containing one row for each textbox in the PDF.
Use `[[1]]` to extract information from the first page.

```{r}
read <- function(pdf.file){
  # read in the pdf document; select the first page 
  pdfData <- pdf_data(pdf.file)
  p1Data <- pdfData[[1]] 
  return(p1Data)

}

evfile <- "Allegany County Statistical Report 2018.pdf"
evdf <- read(evfile)
```

<center>
![](plot/read.png)
</center>
 
We can see that this function splits 
the individual words. "x" and "y" are 
the corresponding column and row value 
respectively so it is pretty easy for us 
to manipulate with them! 
Now we would try to group words 
in the same row together with `group_by()`.

Function [arrange()](https://www.rdocumentation.org
/packages/dplyr/versions/0.7.8/topics/arrange) 
is helpful when we would like to arrange rows
by sorting variables in an descending order. 
`by_group = TRUE` will sort first by grouping variable. 
Year information is easy to get since 
it is list as the first textbox in the dataframe 
and we only need to transfer it as numeric data. 
Considering the county, have a look on the pdf snapshot, 
the full county name is listed in the same row. 
Therefore we decide to paste text in the same row 
together and extract it. Try on test file first!

```{r}
evdf_line <- evdf %>% group_by(y) %>%
  arrange(x, .by_group=TRUE)%>% 
  summarize(line = paste(text, collapse=" "))

head(evdf_line)
```


The second line is what we would like to 
get - full county name. Then convert 
above step into function `ex()`:


```{r}
ex <- function(p1Data){
   # get the year and country from the header
  p1Data <- p1Data %>% group_by(y) %>%
  arrange(x, .by_group=TRUE)
    
  year <- p1Data$text[1] %>% as.numeric()
  
  county <- p1Data %>% 
  summarize(line = paste(text, collapse=" ")) %>%
  slice(2) %>% select(line) %>% as.character()
  
  return(list(county, year))
}


evdf_ex <- ex(evdf)

evdf_ex
```

## Step 2&3: Slice & Split

The first few lines of text don't contain any data, 
but do contain the year and county for the document. 
After extracting year and county, the second step is
to slice out the header part. Our strategy is to find 
where "calls" appear at the first time, that's why we
use `min()` to get its row value. Then just keep rows 
with larger value than it.

We still have the problem that there are really two columns 
worth of data on this page of the pdf, which is reflected 
in row 5 of this line data frame:

```{r}
evdf_line[5,]
```

If we look at the original file they should be 
saved as different columns:

<center>
![](plot/split.png)
</center>

To split the data into two groups by column, we need 
find the minimum column value for the right part and 
any text with smaller column value will be seen as 
the left column. Remember `x` corresponds to the column value.

```{r eval=FALSE}
evdf %>% group_by(x) %>% 
  arrange(y, .by_group=TRUE) 
```

<center>
![](plot/colvalue.png)
</center>

The end of the first column is at x = 258 and 
the begining of the right column is at x = 266. 
We would set x = 265 as the threshold to 
designate left/right column by using function `mutate`. 
Above step will be written as function `mod()`.

```{r}
mod <- function(p1Data){
  
  y.cut <- min(p1Data$y[p1Data$text=="Calls"])
  
  p1Data <- p1Data %>% 
    filter(y > y.cut + 1)%>%
   mutate(column=ifelse(x < 265, "Left", "Right"))
  # create the column variable (Left/Right)
  return(p1Data)
}
```


```{r}
evdf_mod <- mod(evdf)
head(evdf_mod)
```

## Step 4: Collapse line within column

Now, we want to collapse lines within a column 
by first grouping by `column`, then grouping by `y` and 
collapsing across `x`. This just repeats what we did 
when extracting full county name:

```{r}
evdf_line2 <- evdf_mod %>% 
  group_by(column,y) %>% 
  arrange(x, .by_group=TRUE) %>%
  summarize(line = paste(text, collapse=" "))

head(evdf_line2)
```

The problem is that it is not reasonable to collapse across 
all words in a row, because the last text value is 
the count of observations in that category. 
What we should complete is to keep the last text piece 
as the variable's value and use the previous pieces
to make up the variable's name. Suppose there is only 
one text piece, leave it as name.

```{r}
evdf_gp <- evdf_mod%>% 
    group_by(column,y) %>% 
    arrange(x, .by_group=TRUE) %>%
    mutate(type = ifelse(x==max(x) & x==min(x), "name", 
           ifelse(x==max(x), "value", "name"))) 


evdf_gp[1:10,]
```

Once these pieces were labelled appropriately, we can collapse across the values of the variable's name to get both the variable itself and the count for that variable. Still, function `paste()` is helpful to merge things of the same row together:

```{r}
evdf_cd <- evdf_gp%>% 
      summarize(variable = paste(text[type=="name"], collapse=" "), 
              count = ifelse(is_empty(text[type=="value"])==FALSE, 
                           text[type=="value"],"0"))

evdf_cd[1:10,]
```

Finally, there are some lines that are just text and not variables/counts and they can be directly removed, like 'Reason for exposure'.

<center>
![](plot/remove.png)
</center>

Similiarly, write all above as function `gp()`.

```{r update gp}
gp <- function(p1Data){

  gd <- p1Data %>% 
    group_by(column,y) %>% 
    arrange(x, .by_group=TRUE) %>%
    mutate(type = ifelse(x==max(x) & x==min(x), "name", 
                         ifelse(x==max(x), "value", "name"))) 
  
    cd <- gd %>% 
      summarize(variable = paste(text[type=="name"], collapse=" "), 
              count = ifelse(is_empty(text[type=="value"])==FALSE, 
                           text[type=="value"],"0")) %>%
    filter(count != "Calls", count!="exposure", count!="Site",
           count!="Outcome",count!="Center",variable!="Maryland")
  
  return(cd)
  
}
```

Then just combine this dataframe with information for year and county as a row-like dataframe:

```{r}
comb <- function(data, year, county){
  # create the data frame for this county/date
  myRow <- as.data.frame(t(as.numeric(gsub(",","",data$count))))
  names(myRow) <- data$variable
  
  myRow$Year <- year
  myRow$County <- county
  return(myRow)
}
```

Now to make this into a function that is given the pdf file and returns the row of data:

```{r, warning=FALSE}

fna_1 <- function(file){
  p1Data <- read(file)
  county <- ex(p1Data)[[1]]
  year <- ex(p1Data)[[2]]
  
  d <- mod(p1Data)
  d <- gp(d)
  d <- comb(d, year, county)
  return(d)
}

```



Great! Let's test on this file and another two additional files:

```{r eval=FALSE}
fna_1("Prince Georges County Statistical Report 2018.pdf")
fna_1("Allegany County Statistical Report 2018.pdf")
fna_1("Talbot County Statistical Report 2018.pdf")
```

<center>
![](plot/fna1.png)
</center>

There isn't any problem for our wrangling function, which is good! And for each file, `fna_1()` returns a dataframe with size 1*45. Then we can try it on all files and merged all output into one dataframe:

```{r}
d1 <- fna_1(paste0("./data/",files[1]))
D1 <- d1

options(warn=2)
for (i in 2:length(files)) {
  di <- fna_1(paste0("./data/",files[i]))
  D1 <- bind_rows(D1,di)
}
```

The final dataset we could get contains `r nrow(D1)` rows and `r ncol(D1)` columns.

## Step 5: Specify category 

One thing we notice is that there are actually some sub-categories and we definitely wish to have MORE additional columns, and some of the sub-columns are now labeled with NA instead of the appropriate subcategory.

<center>
![](plot/sub-cg.png)
</center>



Like the plot above, "Reason for exposure" contains 3 sub-categories: Unintentional, Intentional and Other. We would like to specify records below them, to achieve this goal, we could update our group function:

The higher categories are:
Left column:
Total human exposures to Reason for exposure, give subcategory "Age:"
Unintentional to Intentional, give subcategory "RFE-Unintent:"
Intentional to Other, give subcategory "RFE-Intent:"
Other to end, give subcategory "RFE-Other:"

Right column: 
Management Site to Medical Outcome, give subcategory "MS"
Medical outcome to end, give subcategory "MO"


```{r update gp 2}

gp_update <- function(p1Data){
  # group the data by column and height on the page
# keep the last entry of that column/height as the value
# assign the remaining entries for that column/height the name
  gd <- p1Data %>% 
    group_by(column,y) %>% 
    arrange(x, .by_group=TRUE) %>%
    mutate(type = ifelse(x==max(x) & x==min(x), "name", 
                         ifelse(x==max(x), "value", "name"))) 
  
    cd <- gd %>% 
      summarize(variable = paste(text[type=="name"], collapse=" "), 
                count=ifelse(is_empty(text[type=="value"])==FALSE, 
                             text[type=="value"],"0"), xmin=min(x))
      
 
y.age.min <- cd$y[cd$variable=="Total human exposures"]
y.age.max <- cd$y[cd$variable=="Reason for"]
y.un.min <- cd$y[cd$variable=="Unintentional"]
y.un.max <- cd$y[cd$variable=="Intentional"]
y.int.min <- cd$y[cd$variable=="Intentional"]

x.min <- cd$xmin[cd$variable=="Total human exposures"]

y.int.max <- cd$y[cd$column=="Left" & cd$variable=="Other" & cd$xmin==x.min]
y.other.min <- cd$y[cd$column=="Left" & cd$variable=="Other" & cd$xmin==x.min]

y.ms.min <- cd$y[cd$variable=="Management"]
y.ms.max <- cd$y[cd$variable=="Medical"]
y.mo.min <- cd$y[cd$variable=="Medical"]

cd <- cd %>%
  mutate(variableSub=ifelse(column=="Left" & y > y.age.min & y < y.age.max, "Age:",
         ifelse(column=="Left" & y > y.un.min & y < y.un.max, "ReasonUn:",
         ifelse(column=="Left" & y > y.int.min & y < y.int.max, "ReasonInt:",
         ifelse(column=="Left" & y > y.other.min, "ReasonOther:",
         ifelse(column=="Right" & y > y.ms.min & y < y.ms.max, "MS:",
         ifelse(column=="Right" & y > y.mo.min, "MO:", "")))))))     
      
    
cd <- cd %>%
  filter(count != "Calls", count!="exposure", count!="Site",
           count!="Outcome",count!="Center",variable!="Maryland") %>%
  mutate(name=paste0(variableSub, variable))
  
  return(cd)
}

comb <- function(data, year, county){
  # create the data frame for this county/date
  myRow <- as.data.frame(t(as.numeric(gsub(",","",data$count))))
  names(myRow) <- data$name
  
  myRow$Year <- year
  myRow$County <- county
  return(myRow)
}

```

With the updated function, we still need to update the final function:

```{r}
fna_2 <- function(file){
  p1Data <- read(file)
  county <- ex(p1Data)[[1]]
  year <- ex(p1Data)[[2]]
  d_mod <- mod(p1Data)
  d_gp <- gp_update(d_mod)
  d <- comb(d_gp, year, county)
  return(d)
}
```

Now, let's try our new function on the pdf files to see if it works well:

```{r}
d1 <- fna_2(paste0("./data/",files[1]))
D2 <- d1

options(warn=2)
for (i in 2:length(files)) {
  di <- fna_2(paste0("./data/",files[i]))
  D2 <- bind_rows(D2,di)
}
```

Here, the size of the final dataset becomes `r nrow(D1)` rwos and `r ncol(D1)` columns, which is different from the result when we apply function fna_1. To be specific, there are MORE additional columns. To get an ideal dataset, next important step is variable cleaning.


## Step 6: Variable cleaning

Before cleaning them, let's check what variables do we have now:

```{r eval=FALSE}
colnames(D2)
```


<center>
![](plot/sub-pb.png)
</center>

There are 6 subcategories which we defined before, and it is clear that there are some problems for each subcategory. The first problem is repeated sub-category, like "Age". This is because for different couties, the same subcategory may include different vairbles, so such kind of variables don't have complete information for all counties; Moreover, unknow information exists in each subcategory but it has different forms, leading to the problem that we get unnecessarily additional columns. 

In the followning part, I will deal the whole dataframe for each subcategory. Since the type of all variable names are string, **regular expression (regexp)** would be a powelful tool. Strings always contain unstructured or semi-structured data, and regexp is considered as a concise language for describing pattern strings. In thi case study, we would like to focus on matching patterns with regexp.


### Regexp Introduction

Function [`str_detect()`](https://www.rdocumentation.org/packages/stringr/versions/1.4.0/topics/str_detect). It is useful to determine if a character vector matches a pattern, and returns a logical vector the same length as the input. 

```{r}
x <- c("apple", "banana", "tomato", "mint", "lemon")
str_detect(x, "e")
```

In this example, for `x`, we would like to find if its string elements contain the word "e", so the result for the first (apple) and fifth (lemon) element is TRUE while for others is FALSE. If we want to subset these two elements, just run the following code:

```{r}
x[str_detect(x, "e")]
```


To prepare you for perfomring the matching procedue, common patterns are introduced here:


```{r}
ex_re = c("Abc", "aabc", "a abc", "abcd", "abbbCde", "acd", "aBcD")
ex_re
```

`^` and `$` finds any string that starts with and ends with certain characters respectively:

```{r}
ex_re[str_detect(ex_re,"^A")]
ex_re[str_detect(ex_re,"e$")]
```

Parentheses `()` create a capturing group with value "Cde", and `.` matches any character:

```{r}
ex_re[str_detect(ex_re, ".(Cde)")]
```

If we want to find strings that have "a" followed by "a" or "B",  `()` means capturing group and `|` means OR operand so we could write:

```{r}
ex_re[str_detect(ex_re, "a(a|B)")]
```

`+` means one or more, if we want to find strings that have "a" followed by one or more "b"

```{r}
ex_re[str_detect(ex_re, "^ab+")]
```


You can also use the combination to generate complex matches. For example, `.+` matches any character one or more times.

Let's try then! The following code is to find string start with "a" and end with "d" or "D", but we don't care about the middle part:

```{r}
ex_re[str_detect(ex_re, "^a.+(d$|D$)")]
```

Considering we have to find different patterns to match the variables that needed to be dealt with, let's define a general function to find colnames with certain pattern with function. Just like the example, first get a logical vector, and then use this vector to extract variables needed.

```{r}
str.dt <- function(pattern){
  x.bool <- str_detect(colnames(D2),pattern)
  x <- colnames(D2)[x.bool]
  return(x)
}
```


### Age

Let's check all variables of Age sub-category:

```{r}
str.dt("Age:")
```

One variable called "Age:<60 years" is pretty wried since it covers lots of other variables. Therefore let's look which observation contain this information:

```{r}
idx <- is.na(D2[str.dt("Age:<60")])
D2[idx==FALSE,c("Age:>60 years","County","Year")]
```

Only Caroline county includes this information and it has no value for "Age:>60 years" which is a common variable for all counties. Thus, we believe "<60" is a typo and it should be ">60" instead.


Then, we find there are Unknow adult, Unknown Adult, Adult, 20-, >60, <60 (typo), it is easy to extact them and check the corresponding summary with the following code:

** First fix the typo into >60 
and then create an overall new variable
and then keep 20-59, >60


```{r}
temp_age <- c(str.dt("Age:(Ad|20-|>60|<60)"),str.dt(".(Adu|adu)"))
summary(D2[temp_age])
```


All of them have a large proportion of NA's. This is because when MPC collected information, it may use different variables to represent the same kind of information. Just like Caroline county, it has information in "<60" but only has NA in ">60". So we decide to sum all their values as a new vairble "Age:>20 years":

```{r}
D2$`Age:>20 years`= rowSums(D2[temp_age],na.rm = TRUE, dims=1)
summary(D2$`Age:>20 years`)
```

`na.rm = TRUE` calculates the sum of the non-NA values in these variables and `dim = 1` tells R to sum over column rather than row.

The new variable has no NA's. Done for the Age part!

### RFE-Unintent

First let's check variables in RFE-Unintent sub-category:

```{r}
str.dt("ReasonUn:")
tempun <- D2[c(str.dt("ReasonUn:"),"County","Year")]
```

Do you notice there are actually repeated information for two variables, "Bite or sting" and "Bite and Sting", "Food Poisoning" and "Food poisoning". The only difference is the upper-case and lower-case but they represent the same thing so use new variables to store their values:

```{r}
str.dt(".(Poi|poi)")
D2$`ReasonUn:Botulism`= rowSums(D2[str.dt(".(Poi|poi)")],na.rm = TRUE, dims=1)
str.dt(".(or Sti|or sti)")
D2$`ReasonUn:Bite/Sting`= rowSums(D2[str.dt(".(or Sti|or sti)")],na.rm = TRUE, dims=1)
```

One thing you should be very careful is how to create the new variable. Since all of the variables we dealt with will be dropped, so do pay attention the new variable has different pattern from these old variables.  


### RFE-Other

Look at variables for RFE-Other sub-category:

```{r}
str.dt("ReasonOther:")
```

One major problem is repeated information for unknow information but we would deal with it in later steps together with all sub-categories.


Another problem is still the upper-case and lower-case difference, so we could dealt with it similiarly. Also
putting space after "/" in new variable as "/ " is intend to avoid deleting the new one in later step:

```{r}
str.dt(".(/Tamp|/tamp).")
D2$`ReasonOther:Contamination/ Tampering`= rowSums(D2[str.dt(".(/Tamp|/tamp).")],na.rm = TRUE, dims=1)
```

### RFE-Intent, MS and MO 

For RFE-Intent, MS and MO sub-category, there are no additional problems except unknow information.

```{r}
str.dt("ReasonInt:")
str.dt("MS:")
str.dt("MO:")
```

### Unknown Information

As we mentioned over and over, one common probelm for each subcategory is multiple similiar variables to represent unknown information, we would like to sum all values in "Unknown" or "Other", and create a new 
" Unknown All" column to store the sum value, here we leave a space for the same reason: avoid to create same pattern

```{r}
D2$`Age: Unknown All`= rowSums(D2[str.dt("^Age.+(ge|wn|ld)$")],na.rm = TRUE, dims=1)
D2$`ReasonUn: Unknown All`= rowSums(D2[str.dt("ReasonUn:(Unk|Oth)")],na.rm = TRUE, dims=1)
D2$`ReasonOther: Unknown All`= rowSums(D2[str.dt("ReasonOther:(Unk|Oth)")],na.rm = TRUE, dims=1)
D2$`MS: Unknown All`= rowSums(D2[str.dt("MS:(Unk|Oth)")],na.rm = TRUE, dims=1)


c(str.dt("^Age.+(ge|wn|ld)$"),str.dt("ReasonUn:(Unk|Oth)"),str.dt("ReasonOther:(Unk|Oth)"),
  str.dt("MS:(Unk|Oth)"))

```

### Additional steps

There are actually 2 wired variables 'Medical','ReasonUn:Outcome', which only has one value;

```{r}
summary(D2[c('Medical','ReasonUn:Outcome')])
```

for "Calvert County, MD, 2017". This is caused by wired format of original pdf, just delete it


<center>
![](plot/calvert.png)
</center>



 
```{r}
idx <- is.na(D2[c('Medical','ReasonUn:Outcome')])

D2[idx==FALSE,c("County","Year")]
```

```{r}
table(idx)["FALSE"]
#idx
```


In all, we make change on 32 variables and now let's drop all old variables and save into a new dataframe

```{r}
drops <- c(str.dt(".(Adu|adu)"), str.dt("Age:(Ad|20-|>60|<60)"),
           str.dt(".(Poi|poi)"), str.dt(".(or Sti|or sti)"), 
           str.dt(".(/Tamp|/tamp)."), str.dt("^Age.+(ge|wn|ld)$"),
           str.dt("ReasonUn:Unk"), str.dt("ReasonOther:(Unk|Oth)"),
           str.dt("MS:(Unk|Oth)"),"Medical",'ReasonUn:Outcome')

Df <- D2[ , !(names(D2) %in% drops)]


#sort(names(Df))
```


Write this clean data to a .csv file:

```{r writeData}
write_csv(Df, "ocs_MPC_data.csv")
```

# Animated map creation

In this section, we would like to take the Maryland Poison Control data that we cleaned, and create a map showing the rates of exposures by county. Additionally, we also want to animate this map to show how these rates change over time. 

## Single map - year 2018


### Data cleaning

To make each step more clear, we use data in year 2018 as an example.

```{r}
map_df <- read_csv("ocs_MPC_data.csv")
### md county line map definition data
md <- map_data('county', 'maryland')
### md state line map definition data
state.md <- map_data('state', 'maryland')
### county population data
popData <- read_csv("pop.csv")


```


Package `ggplot` loads the package `maps` to run function `map_data()`. We mainly use `county` and `state` variable, which provide information of the counties and states of the United States mainland generated from US Department of the Census data respectively. Below is the instructions on how to understand the output:

* `long` is longitude. Things to the west of the prime meridian are negative. And `lat` is latitude.
* `order` shows in which order ggplot should connect the dots.
* `region` and `subregion` tell what region or subregion a set of points surrounds.
* `group` is very important! ggplot2's functions can take a group argument which controls (amongst other things) whether adjacent points should be connected by lines. If they are in the same group, then they get connected, but if they are in different groups then they don't. In our case, each county has its own group number.


Since we would like to merge the dataset based on the `subregion`, it is necessary to change them to be the same format as that in the `md` and `state.md` dataset. First use `tolower` to change all words into lowerer case and then replace some patterns to `""`. Keep in mind that when you define multiple patterns, use regex operator rather vector. When fed with a single pattern, `str_replace_all` will compare that pattern for against every element. However, if you pass it a vector, it will try to respect the order, so compare the first pattern with the first object, then the second pattern with the second object, which is not what we want.


```{r}
### filter to just 2018; get subregions to match
mpc2018 <- map_df %>%
  filter(Year==2018) %>%
  mutate(subregion=tolower(County))

str.rp <- function(str, p){
  str <- str_replace_all(str,p, "")
  return(str)
}

p1 = c(", md| county|[.]|’")
mpc2018$subregion <- str.rp(mpc2018$subregion, p1)

mpc2018$subregion
```

Great! Variable subregion has been reformatted successfully and now let's merge datasets.

```{r}
# join mpc to map data
plotData <- inner_join(md, mpc2018, by="subregion")
```

Now we need to find some population data by county and remove the `, md` and the apostrophes:

```{r}
pop2018 <- popData %>%
  select(`2018`=respop72018, `GEO.display-label`) %>%
  mutate(subregion=tolower(`GEO.display-label`))

p2=c(", maryland| county|'|[.]")

pop2018$subregion = str.rp(pop2018$subregion, p2)

```

### Plotting exposure rates by county

Now we can join all three data sets (MPC, map, and population) together and create an exposure rate variable with exposures per 10,000 people:

```{r}
plot18 <- inner_join(plotData, pop2018, by="subregion")

unique(plot18$subregion)

plot18 <- plot18 %>%
  mutate(THEperCap=`Total human exposures`/`2018`*10000)

```


map in this format can be plotted by function `geom_polygon()`, it draws lines together between points in the same group based on the longtitude and latitude value. `fill`. `coord_fixed()` fixes the relationship between one unit in the y direction and one unit in the x direction, we find 1.3 is the best choice.


```{r}
ggplot() + 
  geom_polygon(data = plot18, aes(x=long, y = lat, 
              fill=THEperCap, group = group), color="white") + 
  geom_polygon(data = state.md, aes(x=long, y=lat, group=group), 
              color="black", fill=NA) + coord_fixed(1.3) + 
  theme_bw() + scale_fill_gradient(low = "white", 
                                   high = "purple", na.value="grey80")

```

PG and M counties do have accurate counts, just very low. If you take a look at the original pdf files for this two counties, it mentions that "This report reflects only the calls to the Maryland Poison Center from Montgomery/ Prince Georges County. For complete statistics, statistics from the National Capitol Poison Center should also be consulted." Thus we decide to exclude these two counties by assigning NA to their rate. 

```{r}
plot18 <- plot18 %>%
  mutate(THEperCap = ifelse(subregion=="prince georges" | 
        subregion=="montgomery", NA, `Total human exposures`/`2018`*10000))


ggplot() + 
  geom_polygon(data = plot18, aes(x=long, y = lat, 
              fill=THEperCap, group = group), color="white") + 
  geom_polygon(data = state.md, aes(x=long, y=lat, group=group), 
              color="black", fill=NA) + coord_fixed(1.3) + 
  theme_bw() + scale_fill_gradient(low = "white", 
                                   high = "purple", na.value="grey80")

```

We can see that Kent and Talbot county (in dark purple at the right part of the map) have much higher numbers of exposures than the other counties in Maryland. Also, after removing the artificially low rates for these two counties, we are also able to see more differentiation between the other counties since the exposure scale has been re-scaled to a new lowest value.

## Animation

### Base exposure rate map

When we create animation, the concept is pretty simple as the motion of an animated plot is driven by a grouped variable in the dataset. Each group will be used to create a single plot, and finally these plots are stitched together as multiple layers to create the animation. 

<center>
![](plot/animate.png)

[https://towardsdatascience.com/animating-your-data-visualizations-like-a-boss-using-r-f94ae20843e3]
</center>

First we select data from 2010 to 2018 and merge necessary information as used in previous example:

```{r}
mpc <- map_df %>%
  filter(Year >= 2010) %>%
  mutate(subregion=tolower(County))

mpc$subregion <- str.rp(mpc$subregion, p1)


colnames(popData) <- str.rp(colnames(popData), "respop7")

popdata <- popData %>%
  select(`2010`:`2018`, subregion=`GEO.display-label`) %>%
  mutate(subregion=str.rp(tolower(subregion),p2)) %>%
  gather(Year, Population, `2010`:`2018`)%>%
  mutate(Year=as.numeric(Year))
  
```

To make the later merging step easier, we reformat the dataset `popData` such that the time variable (`Year`) is gathered together as a single variable rather than spreading across the columns. Function `gather( key=, value=)` takes columns `2010`:`2018` as we specified, and collapses into key-value pairs (`Population`), duplicating all other columns as needed. 

```{r}
# join mpc to map data
join_data <- inner_join(md, mpc, by="subregion")
# join population data to mpc and map data
join_data <- inner_join(join_data, popdata, by=c("subregion", "Year"))

join_data <-  join_data%>%
  mutate(Rate= ifelse(subregion=="prince georges" | subregion=="montgomery", NA,`Total human exposures`/`Population`*10000))

```

Now we get dataset that will be used to create the animation!


```{r}
# to get text of year to show in middle of plot
join_data <- join_data %>%
  mutate(xloc=-78.5, yloc=38.75)

# initial map information
baseMap <- ggplot() + 
  geom_polygon(data = join_data, aes(x=long, y = lat, 
              fill=Rate, group = group), color="white") + 
  geom_polygon(data = state.md, aes(x=long, y=lat, 
              group=group), color="black", fill=NA) + coord_fixed(1.3) +
  scale_fill_gradient(low = "white", high = "purple", na.value="grey80") +
  theme_void() + theme(legend.position=c(.92,.7), legend.text.align=0, 
        plot.title=element_text(hjust=.1, face="bold"), 
        plot.caption=element_text(hjust=0)) +
  labs(title="Total human exposures per 10,000 individuals", 
       fill="Rate per 10k", caption="Data sources: 
       (1) Maryland Poison Center 
       (2) U.S. Census Bureau, Population Division \n 
       Data for Prince George's and Montgomery Counties 
       are excluded since complete data from these counties 
       requires consideration of calls to 
       the National Capitol Poison Center as well.") 

baseMap
```

### Animate exposure rate map over time 

The last step was to animate the graph using the `gganimate` package, which enables you to build motion more much more easily. Again, [an online tutorial](https://towardsdatascience.com/animating-your-data-visualizations-like-a-boss-using-r-f94ae20843e3) inspires us a lot.

Here, `transition_states()` function splits your data into multiple states based on the levels in a given column, corresoinding to multiple layers mentioned above. The relative length of the transition (`transition_length`) and the pause at the states (`state_length`) are defined as 3 and 20 repectively. In addition, function `animate()` takes a gganim object and renders it into an animation.


```{r}
# animate and add year label to animation
animatedMap <- baseMap +
  geom_text(data=join_data, aes(y=yloc, x=xloc, label=as.character(Year)), 
            check_overlap = TRUE, size=10, fontface="bold") +
  transition_states(Year, 3, 20)

# save as gif
mapGIF <- animate(animatedMap) 

# display gif
mapGIF
```

Write the gif image to a file using `anim_save()`:

```{r}
anim_save("MPCmap.gif", animation=mapGIF)
```

# Summary of results


